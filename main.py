import feedparser
import requests
import os
import datetime
import time
from time import mktime
from bs4 import BeautifulSoup

# --- C·∫§U H√åNH ---
TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")

# S·ªê L∆Ø·ª¢NG TIN: 30 tin m·ªói m·ª•c
LIMIT_PER_CAT = 30 
DELETE_LIMIT = 200 # TƒÉng s·ªë l∆∞·ª£ng tin x√≥a c≈© l√™n ƒë·ªÉ ƒë·∫£m b·∫£o s·∫°ch s·∫Ω

# C·∫§U H√åNH DANH M·ª§C & T·ª™ KH√ìA CHUY√äN S√ÇU
DANH_MUC = [
    {
        "ten": "üåç T√ÄI CH√çNH & KINH T·∫æ TH·∫æ GI·ªöI",
        "urls": [
            "https://cafef.vn/tai-chinh-quoc-te.rss",
            "https://vnexpress.net/rss/the-gioi.rss",
            "https://vneconomy.vn/timeline/9920/the-gioi.htm",
            "https://bnews.vn/rss/the-gioi.rss"
        ],
        # T·ª´ kh√≥a bao tr√πm FED, V√†ng, D·∫ßu, Crypto, T·ª∑ gi√°...
        "keywords": [
            "fed", "c·ª•c d·ª± tr·ªØ li√™n bang", "l√£i su·∫•t", "l·∫°m ph√°t", "gdp", "cpi", "pmi",
            "usd", "t·ª∑ gi√°", "y√™n nh·∫≠t", "nh√¢n d√¢n t·ªá", "eur", "v√†ng", "d·∫ßu", "nƒÉng l∆∞·ª£ng",
            "world bank", "imf", "ecb", "suy tho√°i", "kh·ªßng ho·∫£ng", "bitcoin", "crypto",
            "ch·ª©ng kho√°n m·ªπ", "wall street", "dow jones", "nasdaq", "s&p 500",
            "trung qu·ªëc", "kinh t·∫ø m·ªπ", "xu·∫•t kh·∫©u", "chu·ªói cung ·ª©ng"
        ]
    },
    {
        "ten": "üî• ƒê·ªäA CH√çNH TR·ªä & B·∫§T ·ªîN TO√ÄN C·∫¶U",
        "urls": [
            "https://vnexpress.net/rss/the-gioi.rss",
            "https://thanhnien.vn/rss/the-gioi.rss",
            "https://tuoitre.vn/rss/the-gioi.rss"
        ],
        # T·ª´ kh√≥a v·ªÅ xung ƒë·ªôt, qu√¢n s·ª±, b·∫ßu c·ª≠
        "keywords": [
            "xung ƒë·ªôt", "chi·∫øn tranh", "qu√¢n s·ª±", "giao tranh", "t·∫•n c√¥ng", "kh·ªßng b·ªë",
            "bi·ªÉu t√¨nh", "b·∫°o lo·∫°n", "ƒë·∫£o ch√≠nh", "b·∫ßu c·ª≠", "t·ªïng th·ªëng", "th·ªß t∆∞·ªõng",
            "nato", "li√™n h·ª£p qu·ªëc", "bi·ªÉn ƒë√¥ng", "trung ƒë√¥ng", "gaza", "israel", "hamas",
            "ukraine", "nga", "tri·ªÅu ti√™n", "h·∫°t nh√¢n", "t√™n l·ª≠a", "v≈© kh√≠", "bi√™n gi·ªõi",
            "tr·ª´ng ph·∫°t", "c·∫•m v·∫≠n", "ngo·∫°i giao", "houthi", "bi·ªÉn ƒë·ªè"
        ]
    },
    {
        "ten": "üìà CH·ª®NG KHO√ÅN & T√ÄI CH√çNH VI·ªÜT NAM",
        "urls": [
            "https://cafef.vn/tai-chinh-chung-khoan.rss",
            "https://vietstock.vn/rss/chung-khoan.rss",
            "https://tinnhanhchungkhoan.vn/rss/chung-khoan.rss",
            "https://vneconomy.vn/timeline/6/chung-khoan.htm"
        ],
        # T·ª´ kh√≥a ph√¢n t√≠ch, m√£ c·ªï phi·∫øu, b√°o c√°o
        "keywords": [
            "vn-index", "vnindex", "hnx", "upcom", "c·ªï phi·∫øu", "ch·ª©ng kho√°n", "thanh kho·∫£n",
            "kh·ªëi ngo·∫°i", "t·ª± doanh", "l·ª£i nhu·∫≠n", "thua l·ªó", "b√°o c√°o t√†i ch√≠nh", "c·ªï t·ª©c",
            "ng√¢n h√†ng", "b·∫•t ƒë·ªông s·∫£n", "tr√°i phi·∫øu", "ƒë√°o h·∫°n", "v·ªën h√≥a", "ipo",
            "nh·∫≠n ƒë·ªãnh", "ph√¢n t√≠ch", "khuy·∫øn ngh·ªã", "b·∫Øt ƒë√°y", "ch·ªët l·ªùi", "margin",
            "hpg", "vcb", "ssi", "vic", "vhm", "fpt", "mwg" # C√°c m√£ bluechip v√≠ d·ª•
        ]
    },
    {
        "ten": "‚öñÔ∏è CH√çNH S√ÅCH THU·∫æ & LU·∫¨T",
        "urls": [
            "https://thuvienphapluat.vn/rss/van-ban-moi.xml",
            "https://vnexpress.net/rss/phap-luat.rss",
            "https://cafef.vn/vi-mo-dau-tu.rss",
            "https://tapchitaichinh.vn/co-che-chinh-sach.rss"
        ],
        # T·ª´ kh√≥a chuy√™n v·ªÅ Thu·∫ø
        "keywords": [
            "thu·∫ø", "vat", "thu·∫ø thu nh·∫≠p", "thu·∫ø tndn", "thu·∫ø tncn", "ho√†n thu·∫ø",
            "t·ªïng c·ª•c thu·∫ø", "b·ªô t√†i ch√≠nh", "h·∫£i quan", "ngh·ªã ƒë·ªãnh", "th√¥ng t∆∞", "lu·∫≠t",
            "ch√≠nh ph·ªß", "qu·ªëc h·ªôi", "d·ª± th·∫£o", "ban h√†nh", "quy ƒë·ªãnh m·ªõi", "x·ª≠ ph·∫°t",
            "h√≥a ƒë∆°n ƒëi·ªán t·ª≠", "ch√≠nh s√°ch t√†i kh√≥a", "gi·∫£m thu·∫ø", "mi·ªÖn thu·∫ø"
        ]
    },
    {
        "ten": "üõí TH∆Ø∆†NG M·∫†I ƒêI·ªÜN T·ª¨ (E-COM)",
        "urls": [
            "https://cafebiz.vn/cong-nghe.rss",
            "https://vnexpress.net/rss/kinh-doanh.rss",
            "https://vneconomy.vn/timeline/99/tieu-dung.htm"
        ],
        # T·ª´ kh√≥a v·ªÅ E-com, Logistics, B√°n l·∫ª online
        "keywords": [
            "th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠", "e-commerce", "mua s·∫Øm tr·ª±c tuy·∫øn", "online", "b√°n l·∫ª",
            "shopee", "lazada", "tiki", "tiktok shop", "amazon", "alibaba", "temu",
            "giao h√†ng", "logistic", "kho b√£i", "thanh to√°n", "v√≠ ƒëi·ªán t·ª≠", "momo", "zalopay",
            "livestream", "ch·ªët ƒë∆°n", "doanh thu online", "chuy·ªÉn ƒë·ªïi s·ªë"
        ]
    },
    {
        "ten": "üìä S·ªê LI·ªÜU & XU H∆Ø·ªöNG DU L·ªäCH",
        "urls": [
            "https://vnexpress.net/rss/du-lich.rss",
            "https://tcdulichphat.com/rss/home", 
            "https://baodautu.vn/du-lich.rss"
        ],
        # T·ª´ kh√≥a t·∫≠p trung v√†o B√ÅO C√ÅO, S·ªê LI·ªÜU (Lo·∫°i b·ªè tin review ƒÉn ch∆°i)
        "keywords": [
            "s·ªë li·ªáu", "th·ªëng k√™", "b√°o c√°o", "doanh thu", "l∆∞·ª£t kh√°ch", "kh√°ch qu·ªëc t·∫ø",
            "kh√°ch n·ªôi ƒë·ªãa", "l·ªØ h√†nh", "h√†ng kh√¥ng", "v√© m√°y bay", "c·ª•c du l·ªãch", "visa",
            "th·ªã th·ª±c", "mi·ªÖn visa", "xu h∆∞·ªõng", "c√¥ng su·∫•t ph√≤ng", "kh√°ch s·∫°n", "resort",
            "du l·ªãch b·ªÅn v·ªØng", "mice", "tƒÉng tr∆∞·ªüng", "s·ª•t gi·∫£m", "un tourism"
        ]
    }
]

def clean_html(raw_html):
    # L√†m s·∫°ch th·∫ª HTML nh∆∞ng gi·ªØ l·∫°i text d√†i
    try:
        soup = BeautifulSoup(raw_html, "lxml")
        text = soup.get_text(separator=" ").strip()
        # X√≥a c√°c c·ª•m t·ª´ th·ª´a th∆∞·ªùng g·∫∑p trong RSS
        text = text.replace("TTO -", "").replace("(D√¢n tr√≠)", "").strip()
        return text
    except:
        return raw_html

def convert_time(entry):
    try:
        if hasattr(entry, 'published_parsed'):
            dt_utc = datetime.datetime.fromtimestamp(mktime(entry.published_parsed))
            dt_vn = dt_utc + datetime.timedelta(hours=7)
            return dt_vn.strftime("%H:%M")
    except: pass
    return "M·ªõi"

def don_dep_chat():
    print("üßπ ƒêang d·ªçn d·∫πp s·∫°ch s·∫Ω tin c≈©...")
    url_send = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    try:
        # G·ª≠i tin m·ªìi
        resp = requests.post(url_send, json={"chat_id": TELEGRAM_CHAT_ID, "text": "‚è≥ ƒêang t·ªïng h·ª£p 180 tin t·ª©c..."}).json()
        if not resp.get("ok"): return

        current_id = resp['result']['message_id']
        # X√≥a ng∆∞·ª£c v·ªÅ qu√° kh·ª©
        for i in range(current_id, current_id - DELETE_LIMIT, -1):
            url_del = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/deleteMessage"
            requests.post(url_del, json={"chat_id": TELEGRAM_CHAT_ID, "message_id": i})
            
    except Exception as e:
        print(f"L·ªói d·ªçn d·∫πp: {e}")

def gui_theo_lo(ds_msg):
    # H√†m g·ª≠i tin nh·∫Øn, t·ª± ƒë·ªông chia nh·ªè n·∫øu qu√° d√†i
    for msg in ds_msg:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
        
        # N·∫øu tin nh·∫Øn qu√° d√†i (Telegram gi·ªõi h·∫°n 4096 k√Ω t·ª±), c·∫Øt ƒë√¥i ra
        if len(msg) > 4000:
            parts = [msg[i:i+4000] for i in range(0, len(msg), 4000)]
            for part in parts:
                requests.post(url, json={
                    "chat_id": TELEGRAM_CHAT_ID, 
                    "text": part, 
                    "disable_web_page_preview": True,
                    "parse_mode": "Markdown"
                })
                time.sleep(1) # Ngh·ªâ x√≠u tr√°nh b·ªã spam
        else:
            requests.post(url, json={
                "chat_id": TELEGRAM_CHAT_ID, 
                "text": msg, 
                "disable_web_page_preview": True,
                "parse_mode": "Markdown"
            })
            time.sleep(1)

def xu_ly_tin_tuc():
    ngay = datetime.datetime.now().strftime("%d/%m/%Y")
    messages_queue = []
    
    # Header ƒë·∫ßu ti√™n
    messages_queue.append(f"üìÖ **B√ÅO C√ÅO TO√ÄN C·∫¢NH NG√ÄY {ngay}**\n_Ch·∫ø ƒë·ªô: 30 tin/m·ª•c - M√¥ t·∫£ chi ti·∫øt_")
    
    current_msg = ""
    
    for muc in DANH_MUC:
        header = f"\n‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ\n**{muc['ten']}**\n"
        
        # N·∫øu ƒëang gom d·ªü m√† th√™m header b·ªã d√†i qu√° th√¨ ng·∫Øt lu√¥n
        if len(current_msg) + len(header) > 3000:
            messages_queue.append(current_msg)
            current_msg = header
        else:
            current_msg += header
            
        count = 0
        collected_links = set()
        
        for url in muc['urls']:
            if count >= LIMIT_PER_CAT: break
            try:
                feed = feedparser.parse(url)
                for entry in feed.entries:
                    if count >= LIMIT_PER_CAT: break
                    link = entry.link
                    if link in collected_links: continue
                    
                    # L·ªçc t·ª´ kh√≥a
                    keywords = muc.get('keywords', [])
                    desc_raw = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                    desc_clean = clean_html(desc_raw) # L·∫•y to√†n b·ªô m√¥ t·∫£
                    
                    # Ki·ªÉm tra t·ª´ kh√≥a
                    if keywords:
                        text_check = (entry.title + " " + desc_clean).lower()
                        if not any(k in text_check for k in keywords): continue
                    
                    time_str = convert_time(entry)
                    
                    # N·ªôi dung tin (Title in ƒë·∫≠m, M√¥ t·∫£ ƒë·ªÉ th∆∞·ªùng cho d·ªÖ ƒë·ªçc)
                    news_item = f"\nüïí `{time_str}` | **{entry.title}**\n{desc_clean}\nüëâ [Xem chi ti·∫øt]({link})\n"
                    
                    # Ki·ªÉm tra ƒë·ªô d√†i an to√†n (3500 k√Ω t·ª± ƒë·ªÉ tr·ª´ hao)
                    if len(current_msg) + len(news_item) > 3500:
                        messages_queue.append(current_msg)
                        current_msg = news_item # B·∫Øt ƒë·∫ßu tin m·ªõi
                    else:
                        current_msg += news_item
                    
                    collected_links.add(link)
                    count += 1
            except: pass
            
        if count == 0:
            current_msg += "\n_(Kh√¥ng c√≥ d·ªØ li·ªáu ph√π h·ª£p)_\n"

    # ƒê·∫©y n·ªët ph·∫ßn c√≤n d∆∞
    if current_msg:
        messages_queue.append(current_msg)
        
    return messages_queue

def main():
    if not TELEGRAM_TOKEN:
        print("Ch∆∞a c·∫•u h√¨nh Token!")
        return
    
    # 1. D·ªçn d·∫πp
    don_dep_chat()
    
    # 2. X·ª≠ l√Ω
    ds_tin = xu_ly_tin_tuc()
    
    # 3. G·ª≠i
    gui_theo_lo(ds_tin)

if __name__ == "__main__":
    main()
