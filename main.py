import feedparser
import requests
import os
import datetime
import json
from bs4 import BeautifulSoup
from time import mktime

# --- C·∫§U H√åNH ---
TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")
HISTORY_FILE = "history.json" # Cu·ªën s·ªï tay ghi nh·ªõ

LIMIT_PER_CAT = 15 # S·ªë tin m·ªói m·ª•c

DANH_MUC = [
    {
        "ten": "üåç T√ÄI CH√çNH & KINH T·∫æ TG",
        "urls": ["https://cafef.vn/tai-chinh-quoc-te.rss", "https://vnexpress.net/rss/the-gioi.rss"],
        "keywords": ["kinh t·∫ø", "t√†i ch√≠nh", "fed", "l√£i su·∫•t", "l·∫°m ph√°t", "usd", "t·ª∑ gi√°", "trung qu·ªëc", "m·ªπ", "eu"]
    },
    {
        "ten": "üî• ƒê·ªäA CH√çNH TR·ªä & XUNG ƒê·ªòT",
        "urls": ["https://vnexpress.net/rss/the-gioi.rss", "https://thanhnien.vn/rss/the-gioi.rss"],
        "keywords": ["xung ƒë·ªôt", "chi·∫øn tranh", "qu√¢n s·ª±", "bi·ªÉu t√¨nh", "b·∫ßu c·ª≠", "t·ªïng th·ªëng", "v≈© kh√≠", "nato", "bi·ªÉn ƒë√¥ng", "israel", "nga", "ukraine"]
    },
    {
        "ten": "üìà CH·ª®NG KHO√ÅN & T√ÄI CH√çNH VN",
        "urls": ["https://cafef.vn/tai-chinh-chung-khoan.rss", "https://vietstock.vn/rss/chung-khoan.rss"],
        "keywords": ["c·ªï phi·∫øu", "vn-index", "ch·ª©ng kho√°n", "ng√¢n h√†ng", "l·ª£i nhu·∫≠n", "thua l·ªó", "tr√°i phi·∫øu"]
    },
    {
        "ten": "‚öñÔ∏è THU·∫æ & CH√çNH S√ÅCH M·ªöI",
        "urls": ["https://thuvienphapluat.vn/rss/van-ban-moi.xml", "https://vnexpress.net/rss/phap-luat.rss"],
        "keywords": ["thu·∫ø", "ngh·ªã ƒë·ªãnh", "th√¥ng t∆∞", "lu·∫≠t", "ch√≠nh ph·ªß", "ƒë·ªÅ xu·∫•t", "ban h√†nh"]
    },
    {
        "ten": "üõí TH∆Ø∆†NG M·∫†I ƒêI·ªÜN T·ª¨ (E-COM)",
        "urls": ["https://cafebiz.vn/cong-nghe.rss"],
        "keywords": ["shopee", "lazada", "tiki", "tiktok", "th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠", "online", "b√°n l·∫ª"]
    },
    {
        "ten": "‚úàÔ∏è DU L·ªäCH & XU H∆Ø·ªöNG",
        "urls": ["https://vnexpress.net/rss/du-lich.rss"],
        "keywords": [] 
    }
]

def clean_html(raw_html):
    try:
        soup = BeautifulSoup(raw_html, "html.parser")
        return soup.get_text(separator=" ").strip()
    except:
        return raw_html

def convert_time(entry):
    # H√†m chuy·ªÉn ƒë·ªïi gi·ªù RSS sang gi·ªù Vi·ªát Nam (UTC+7)
    try:
        if hasattr(entry, 'published_parsed'):
            # L·∫•y gi·ªù g·ªëc (UTC)
            dt_utc = datetime.datetime.fromtimestamp(mktime(entry.published_parsed))
            # C·ªông th√™m 7 ti·∫øng
            dt_vn = dt_utc + datetime.timedelta(hours=7)
            return dt_vn.strftime("%H:%M") # Tr·∫£ v·ªÅ d·∫°ng 14:30
    except:
        pass
    return "M·ªõi"

def xoa_tin_nhan_cu():
    # ƒê·ªçc file l·ªãch s·ª≠ ƒë·ªÉ x√≥a tin h√¥m qua
    if not os.path.exists(HISTORY_FILE):
        return
    
    try:
        with open(HISTORY_FILE, 'r') as f:
            old_ids = json.load(f)
            
        print(f"ƒêang x√≥a {len(old_ids)} tin nh·∫Øn c≈©...")
        for msg_id in old_ids:
            url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/deleteMessage"
            requests.post(url, json={"chat_id": TELEGRAM_CHAT_ID, "message_id": msg_id})
            
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc/x√≥a l·ªãch s·ª≠: {e}")

def gui_va_luu_id(ds_tin_nhan):
    # G·ª≠i tin m·ªõi v√† l∆∞u l·∫°i ID c·ªßa ch√∫ng
    sent_ids = []
    
    for msg in ds_tin_nhan:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
        data = {
            "chat_id": TELEGRAM_CHAT_ID, 
            "text": msg, 
            "disable_web_page_preview": True,
            "parse_mode": "Markdown" # ƒê·ªÉ hi·ªÉn th·ªã in ƒë·∫≠m
        }
        try:
            response = requests.post(url, json=data)
            resp_data = response.json()
            if resp_data.get("ok"):
                # L∆∞u l·∫°i ID c·ªßa tin nh·∫Øn v·ª´a g·ª≠i
                sent_ids.append(resp_data["result"]["message_id"])
        except Exception as e:
            print(f"L·ªói g·ª≠i tin: {e}")

    # Ghi ƒë√® v√†o file history.json cho ng√†y mai d√πng
    with open(HISTORY_FILE, 'w') as f:
        json.dump(sent_ids, f)
    print(f"ƒê√£ l∆∞u {len(sent_ids)} ID tin nh·∫Øn v√†o s·ªï tay.")

def xu_ly_tin_tuc():
    ngay = datetime.datetime.now().strftime("%d/%m/%Y")
    messages_queue = [f"üìÖ **B·∫¢N TIN NG√ÄY {ngay}**"]
    current_msg = ""
    
    for muc in DANH_MUC:
        header = f"\n‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ\n**{muc['ten']}**\n"
        
        if len(current_msg) + len(header) > 3500:
            messages_queue.append(current_msg)
            current_msg = header
        else:
            current_msg += header
            
        count = 0
        collected_links = set()
        
        for url in muc['urls']:
            if count >= LIMIT_PER_CAT: break
            try:
                feed = feedparser.parse(url)
                for entry in feed.entries:
                    if count >= LIMIT_PER_CAT: break
                    
                    title = entry.title
                    link = entry.link
                    
                    if link in collected_links: continue
                    
                    # L·ªçc t·ª´ kh√≥a
                    keywords = muc.get('keywords', [])
                    desc_raw = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                    desc_clean = clean_html(desc_raw)
                    
                    if keywords:
                        text_check = (title + " " + desc_clean).lower()
                        if not any(k in text_check for k in keywords):
                            continue
                    
                    # L·∫•y gi·ªù
                    time_str = convert_time(entry)
                    
                    # T·∫°o tin nh·∫Øn c√≥ Gi·ªù
                    news_item = f"\nüïí `{time_str}` | [{title}]({link})\n"
                    
                    if len(current_msg) + len(news_item) > 3800:
                        messages_queue.append(current_msg)
                        current_msg = news_item
                    else:
                        current_msg += news_item
                    
                    collected_links.add(link)
                    count += 1
            except: pass
            
        if count == 0:
            current_msg += "\n_(Kh√¥ng c√≥ tin m·ªõi)_\n"

    if current_msg:
        messages_queue.append(current_msg)
        
    return messages_queue

def main():
    if not TELEGRAM_TOKEN:
        print("Ch∆∞a c·∫•u h√¨nh Token!")
        return
    
    # 1. X√≥a tin c≈© c·ªßa ng√†y h√¥m qua
    xoa_tin_nhan_cu()
    
    # 2. T·∫°o tin m·ªõi
    ds_tin = xu_ly_tin_tuc()
    
    # 3. G·ª≠i v√† l∆∞u ID m·ªõi v√†o s·ªï
    gui_va_luu_id(ds_tin)

if __name__ == "__main__":
    main()
