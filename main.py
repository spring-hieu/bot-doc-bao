import feedparser
import requests
import os
import datetime
import time
from time import mktime
from bs4 import BeautifulSoup

# --- C·∫§U H√åNH ---
TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")

LIMIT_PER_CAT = 30 
DELETE_LIMIT = 200 

# --- C·∫§U H√åNH DANH M·ª§C V√Ä LINK RSS (ƒê√É C·∫¨P NH·∫¨T THEO Y√äU C·∫¶U) ---
DANH_MUC = [
    {
        "ten": "üåç T√ÄI CH√çNH & KINH T·∫æ TH·∫æ GI·ªöI",
        "urls": [
            "https://vietstock.vn/773/the-gioi/chung-khoan-the-gioi.rss",
            "https://vietstock.vn/772/the-gioi/tai-chinh-quoc-te.rss",
            "https://vnexpress.net/rss/the-gioi.rss"
        ],
        "keywords": [
            "fed", "c·ª•c d·ª± tr·ªØ", "l√£i su·∫•t", "l·∫°m ph√°t", "gdp", "cpi", "pmi",
            "usd", "t·ª∑ gi√°", "y√™n nh·∫≠t", "nh√¢n d√¢n t·ªá", "eur", "v√†ng", "d·∫ßu", "nƒÉng l∆∞·ª£ng",
            "world bank", "imf", "ecb", "suy tho√°i", "kh·ªßng ho·∫£ng", "bitcoin", "crypto",
            "ch·ª©ng kho√°n m·ªπ", "wall street", "dow jones", "nasdaq", "s&p 500",
            "trung qu·ªëc", "kinh t·∫ø m·ªπ", "xu·∫•t kh·∫©u", "chu·ªói cung ·ª©ng"
        ]
    },
    {
        "ten": "üî• ƒê·ªäA CH√çNH TR·ªä & B·∫§T ·ªîN TO√ÄN C·∫¶U",
        # Gi·ªØ l·∫°i ngu·ªìn tin th·∫ø gi·ªõi uy t√≠n ƒë·ªÉ l·ªçc tin chi·∫øn s·ª±
        "urls": [
            "https://vnexpress.net/rss/the-gioi.rss",
            "https://tuoitre.vn/rss/the-gioi.rss",
            "https://thanhnien.vn/rss/the-gioi.rss"
        ],
        "keywords": [
            "xung ƒë·ªôt", "chi·∫øn tranh", "qu√¢n s·ª±", "giao tranh", "t·∫•n c√¥ng", "kh·ªßng b·ªë",
            "bi·ªÉu t√¨nh", "b·∫°o lo·∫°n", "ƒë·∫£o ch√≠nh", "b·∫ßu c·ª≠", "t·ªïng th·ªëng", "th·ªß t∆∞·ªõng",
            "nato", "li√™n h·ª£p qu·ªëc", "bi·ªÉn ƒë√¥ng", "trung ƒë√¥ng", "gaza", "israel", "hamas",
            "ukraine", "nga", "tri·ªÅu ti√™n", "h·∫°t nh√¢n", "t√™n l·ª≠a", "v≈© kh√≠", "bi√™n gi·ªõi",
            "tr·ª´ng ph·∫°t", "c·∫•m v·∫≠n", "ngo·∫°i giao", "houthi", "bi·ªÉn ƒë·ªè"
        ]
    },
    {
        "ten": "üìà CH·ª®NG KHO√ÅN & T√ÄI CH√çNH VI·ªÜT NAM",
        "urls": [
            "https://vietstock.vn/830/chung-khoan/co-phieu.rss",
            "https://vietstock.vn/3358/chung-khoan/etf-va-cac-quy.rss",
            "https://vietstock.vn/761/kinh-te/vi-mo.rss",
            "https://vietstock.vn/757/tai-chinh/ngan-hang.rss",
            "https://vietstock.vn/737/doanh-nghiep/hoat-dong-kinh-doanh.rss",
            "https://vietstock.vn/759/hang-hoa/vang-va-kim-loai-quy.rss",
            "https://vietstock.vn/1636/nhan-dinh-phan-tich/nhan-dinh-thi-truong.rss",
            "https://vietstock.vn/582/nhan-dinh-phan-tich/phan-tich-co-ban.rss",
            "https://vietstock.vn/585/nhan-dinh-phan-tich/phan-tich-ky-thuat.rss"
        ],
        "keywords": [
            "vn-index", "vnindex", "hnx", "upcom", "c·ªï phi·∫øu", "ch·ª©ng kho√°n", "thanh kho·∫£n",
            "kh·ªëi ngo·∫°i", "t·ª± doanh", "l·ª£i nhu·∫≠n", "thua l·ªó", "b√°o c√°o t√†i ch√≠nh", "c·ªï t·ª©c",
            "ng√¢n h√†ng", "b·∫•t ƒë·ªông s·∫£n", "tr√°i phi·∫øu", "ƒë√°o h·∫°n", "v·ªën h√≥a", "ipo",
            "nh·∫≠n ƒë·ªãnh", "ph√¢n t√≠ch", "khuy·∫øn ngh·ªã", "b·∫Øt ƒë√°y", "ch·ªët l·ªùi", "margin",
            "k·ªπ thu·∫≠t", "c∆° b·∫£n", "etf", "qu·ªπ", "v√†ng", "sjc"
        ]
    },
    {
        "ten": "‚öñÔ∏è CH√çNH S√ÅCH THU·∫æ & LU·∫¨T",
        "urls": [
            "https://thuvienphapluat.vn/rss.xml", # Link t·ªïng h·ª£p
            "https://vnexpress.net/rss/phap-luat.rss",
            "https://dantri.com.vn/rss/phap-luat.rss"
        ],
        "keywords": [
            "thu·∫ø", "vat", "thu·∫ø thu nh·∫≠p", "thu·∫ø tndn", "thu·∫ø tncn", "ho√†n thu·∫ø",
            "t·ªïng c·ª•c thu·∫ø", "b·ªô t√†i ch√≠nh", "h·∫£i quan", "ngh·ªã ƒë·ªãnh", "th√¥ng t∆∞", "lu·∫≠t",
            "ch√≠nh ph·ªß", "qu·ªëc h·ªôi", "d·ª± th·∫£o", "ban h√†nh", "quy ƒë·ªãnh m·ªõi", "x·ª≠ ph·∫°t",
            "h√≥a ƒë∆°n ƒëi·ªán t·ª≠", "ch√≠nh s√°ch t√†i kh√≥a", "gi·∫£m thu·∫ø", "mi·ªÖn thu·∫ø"
        ]
    },
    {
        "ten": "üõí TH∆Ø∆†NG M·∫†I & KINH DOANH ONLINE",
        "urls": [
            "https://vnexpress.net/rss/kinh-doanh.rss",
            "https://tinhte.vn/rss" # Link n√†y nhi·ªÅu tin c√¥ng ngh·ªá, c·∫ßn l·ªçc k·ªπ
        ],
        "keywords": [
            "th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠", "e-commerce", "mua s·∫Øm tr·ª±c tuy·∫øn", "online", "b√°n l·∫ª",
            "shopee", "lazada", "tiki", "tiktok shop", "amazon", "alibaba", "temu",
            "giao h√†ng", "logistic", "kho b√£i", "thanh to√°n", "v√≠ ƒëi·ªán t·ª≠", "momo", "zalopay",
            "livestream", "ch·ªët ƒë∆°n", "doanh thu online", "chuy·ªÉn ƒë·ªïi s·ªë"
        ]
    },
    {
        "ten": "üìä S·ªê LI·ªÜU & XU H∆Ø·ªöNG DU L·ªäCH",
        "urls": [
            "https://thanhnien.vn/rss/du-lich.rss", 
            "https://dantri.com.vn/rss/du-lich.rss", 
            "https://tuoitre.vn/rss/du-lich.rss"
        ],
        "keywords": [
            "s·ªë li·ªáu", "th·ªëng k√™", "b√°o c√°o", "doanh thu", "l∆∞·ª£t kh√°ch", "kh√°ch qu·ªëc t·∫ø",
            "kh√°ch n·ªôi ƒë·ªãa", "l·ªØ h√†nh", "h√†ng kh√¥ng", "v√© m√°y bay", "c·ª•c du l·ªãch", "visa",
            "th·ªã th·ª±c", "mi·ªÖn visa", "xu h∆∞·ªõng", "c√¥ng su·∫•t ph√≤ng", "kh√°ch s·∫°n", "resort",
            "du l·ªãch b·ªÅn v·ªØng", "mice", "tƒÉng tr∆∞·ªüng", "s·ª•t gi·∫£m", "un tourism"
        ]
    }
]

def clean_html(raw_html):
    try:
        soup = BeautifulSoup(raw_html, "html.parser")
        # X√≥a h·∫øt ·∫£nh, link, video ƒë·ªÉ tin nh·∫Øn g·ªçn g√†ng
        for tag in soup(['script', 'style', 'img', 'iframe', 'video', 'a']):
            tag.decompose()
        
        text = soup.get_text(separator=" ")
        text = " ".join(text.split())
        
        # X√≥a c√°c c·ª•m t·ª´ th·ª´a
        garbage_phrases = ["TTO -", "(D√¢n tr√≠)", "VTV.vn -", "B√°o ƒê·∫ßu t∆∞ -", "ANTD.VN -"]
        for phrase in garbage_phrases:
            text = text.replace(phrase, "")
            
        return text.strip()
    except:
        return ""

def convert_time(entry):
    try:
        if hasattr(entry, 'published_parsed'):
            dt_utc = datetime.datetime.fromtimestamp(mktime(entry.published_parsed))
            dt_vn = dt_utc + datetime.timedelta(hours=7)
            return dt_vn.strftime("%H:%M")
    except: pass
    return "M·ªõi"

def don_dep_chat():
    print("üßπ B·∫Øt ƒë·∫ßu d·ªçn d·∫πp...")
    url_send = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    try:
        resp = requests.post(url_send, json={"chat_id": TELEGRAM_CHAT_ID, "text": "‚è≥ ƒêang t·ªïng h·ª£p d·ªØ li·ªáu t·ª´ Vietstock & RSS..."}).json()
        if not resp.get("ok"): return

        current_id = resp['result']['message_id']
        # X√≥a 200 tin g·∫ßn nh·∫•t
        for i in range(current_id, current_id - DELETE_LIMIT, -1):
            url_del = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/deleteMessage"
            requests.post(url_del, json={"chat_id": TELEGRAM_CHAT_ID, "message_id": i})
    except Exception as e:
        print(f"L·ªói d·ªçn d·∫πp: {e}")

def gui_theo_lo(ds_msg):
    for msg in ds_msg:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
        # C·∫Øt nh·ªè tin nh·∫Øn n·∫øu qu√° d√†i
        if len(msg) > 4000:
            parts = [msg[i:i+4000] for i in range(0, len(msg), 4000)]
            for part in parts:
                requests.post(url, json={"chat_id": TELEGRAM_CHAT_ID, "text": part, "disable_web_page_preview": True, "parse_mode": "Markdown"})
                time.sleep(1)
        else:
            requests.post(url, json={"chat_id": TELEGRAM_CHAT_ID, "text": msg, "disable_web_page_preview": True, "parse_mode": "Markdown"})
            time.sleep(1)

def xu_ly_tin_tuc():
    ngay = datetime.datetime.now().strftime("%d/%m/%Y")
    messages_queue = []
    messages_queue.append(f"üìÖ **B·∫¢N TIN NG√ÄY {ngay}**")
    
    current_msg = ""
    
    for muc in DANH_MUC:
        header = f"\n‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ\n**{muc['ten']}**\n"
        if len(current_msg) + len(header) > 3000:
            messages_queue.append(current_msg)
            current_msg = header
        else:
            current_msg += header
            
        count = 0
        collected_links = set()
        
        for url in muc['urls']:
            if count >= LIMIT_PER_CAT: break
            try:
                feed = feedparser.parse(url)
                if not feed.entries: continue 

                for entry in feed.entries:
                    if count >= LIMIT_PER_CAT: break
                    link = entry.link
                    if link in collected_links: continue
                    
                    keywords = muc.get('keywords', [])
                    desc_raw = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                    desc_clean = clean_html(desc_raw)
                    
                    if keywords:
                        text_check = (entry.title + " " + desc_clean).lower()
                        if not any(k in text_check for k in keywords): continue
                    
                    time_str = convert_time(entry)
                    
                    # N·ªôi dung tin hi·ªÉn th·ªã
                    news_item = f"\nüïí `{time_str}` | **{entry.title}**\n_{desc_clean}_\nüëâ [Xem chi ti·∫øt]({link})\n"
                    
                    if len(current_msg) + len(news_item) > 3500:
                        messages_queue.append(current_msg)
                        current_msg = news_item 
                    else:
                        current_msg += news_item
                    
                    collected_links.add(link)
                    count += 1
            except Exception as e:
                print(f"L·ªói ƒë·ªçc RSS {url}: {e}")
            
        if count == 0:
            current_msg += "\n_(Ch∆∞a c√≥ tin m·ªõi ph√π h·ª£p)_\n"

    if current_msg:
        messages_queue.append(current_msg)
        
    return messages_queue

def main():
    if not TELEGRAM_TOKEN: return
    don_dep_chat()
    ds_tin = xu_ly_tin_tuc()
    gui_theo_lo(ds_tin)

if __name__ == "__main__":
    main()
